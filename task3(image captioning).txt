# ============================================
# CLEAN IMAGE CAPTIONING (NO pip messages)
# Upload ‚Üí Display ‚Üí Caption
# ============================================

# Suppress pip output
import sys
import subprocess

def install(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# Install required packages silently
install("transformers")
install("torch")
install("torchvision")
install("pillow")

# ---------------------------
# Imports
# ---------------------------
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
from google.colab import files
from IPython.display import display
import torch

# ---------------------------
# Load Pre-trained Model
# ---------------------------
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to("cpu")

# ---------------------------
# Upload Image
# ---------------------------
print("üì§ Please upload an image:")
uploaded = files.upload()
image_path = list(uploaded.keys())[0]

# Display uploaded image
image = Image.open(image_path).convert("RGB")
print("\nüñºÔ∏è Uploaded Image:")
display(image)

# ---------------------------
# Generate Caption
# ---------------------------
inputs = processor(image, return_tensors="pt")
with torch.no_grad():
    out = model.generate(**inputs, max_new_tokens=30)

caption = processor.decode(out[0], skip_special_tokens=True)

# Print final caption
print("\nüìù Generated Caption:")
print(caption)
